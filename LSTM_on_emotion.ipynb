{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-05T15:06:15.715954Z","iopub.execute_input":"2023-06-05T15:06:15.716501Z","iopub.status.idle":"2023-06-05T15:06:15.731924Z","shell.execute_reply.started":"2023-06-05T15:06:15.716455Z","shell.execute_reply":"2023-06-05T15:06:15.730692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing Libraries\n#!pip install nlp\n#!pip install datasets\nimport tensorflow as tf\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nlp\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Flatten\nfrom keras.layers import Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import GlobalAvgPool1D","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:30:47.260209Z","iopub.execute_input":"2023-06-05T14:30:47.260626Z","iopub.status.idle":"2023-06-05T14:31:15.865817Z","shell.execute_reply.started":"2023-06-05T14:30:47.260587Z","shell.execute_reply":"2023-06-05T14:31:15.864357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/emotion-dataset/training.csv\")\ntest = pd.read_csv(\"/kaggle/input/emotion-dataset/test.csv\")\nval = pd.read_csv(\"/kaggle/input/emotion-dataset/validation.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:17:45.577666Z","iopub.execute_input":"2023-06-05T14:17:45.578127Z","iopub.status.idle":"2023-06-05T14:17:45.698370Z","shell.execute_reply.started":"2023-06-05T14:17:45.578097Z","shell.execute_reply":"2023-06-05T14:17:45.697498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:18:40.775796Z","iopub.execute_input":"2023-06-05T14:18:40.776193Z","iopub.status.idle":"2023-06-05T14:18:40.799705Z","shell.execute_reply.started":"2023-06-05T14:18:40.776162Z","shell.execute_reply":"2023-06-05T14:18:40.798697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_dict = {0:'sadness', 1:'joy', 2:'love', 3:'anger', 4:'fear', 5:'surprise'}\ntrain['description'] = train['label'].map(labels_dict )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:19:21.082135Z","iopub.execute_input":"2023-06-05T14:19:21.083149Z","iopub.status.idle":"2023-06-05T14:19:21.109878Z","shell.execute_reply.started":"2023-06-05T14:19:21.083103Z","shell.execute_reply":"2023-06-05T14:19:21.108566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing with NLTK","metadata":{}},{"cell_type":"code","source":"def tokenization(inputs):\n    return word_tokenize(inputs) #REFERENCE[1]\n\n\ntrain['text_tokenized'] = train['text'].apply(tokenization)\nval['text_tokenized'] = val['text'].apply(tokenization)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:25:16.102053Z","iopub.execute_input":"2023-06-05T14:25:16.102853Z","iopub.status.idle":"2023-06-05T14:25:20.470873Z","shell.execute_reply.started":"2023-06-05T14:25:16.102817Z","shell.execute_reply":"2023-06-05T14:25:20.470020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:25:25.205937Z","iopub.execute_input":"2023-06-05T14:25:25.206364Z","iopub.status.idle":"2023-06-05T14:25:25.222518Z","shell.execute_reply.started":"2023-06-05T14:25:25.206335Z","shell.execute_reply":"2023-06-05T14:25:25.221291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using tokenization, I split each data point into words. Tokenization is one of the key steps for NLP applications.","metadata":{}},{"cell_type":"markdown","source":"# Stopwords Removal","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef stopwords_remove(inputs):\n    return [item for item in inputs if item not in stop_words]\n\ntrain['text_stop'] = train['text_tokenized'].apply(stopwords_remove)\nval['text_stop'] = val['text_tokenized'].apply(stopwords_remove)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:26:58.657020Z","iopub.execute_input":"2023-06-05T14:26:58.657525Z","iopub.status.idle":"2023-06-05T14:26:58.756246Z","shell.execute_reply.started":"2023-06-05T14:26:58.657490Z","shell.execute_reply":"2023-06-05T14:26:58.755084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization","metadata":{}},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:35:28.417091Z","iopub.execute_input":"2023-06-05T14:35:28.417567Z","iopub.status.idle":"2023-06-05T14:35:29.944444Z","shell.execute_reply.started":"2023-06-05T14:35:28.417537Z","shell.execute_reply":"2023-06-05T14:35:29.943010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef lemmatization(inputs):\n    return [lemmatizer.lemmatize(word=x, pos='v') for x in inputs]\n\ntrain['text_lemmatized'] = train['text_stop'].apply(lemmatization)\nval['text_lemmatized'] = val['text_stop'].apply(lemmatization)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:35:35.477952Z","iopub.execute_input":"2023-06-05T14:35:35.479668Z","iopub.status.idle":"2023-06-05T14:35:39.570296Z","shell.execute_reply.started":"2023-06-05T14:35:35.479606Z","shell.execute_reply":"2023-06-05T14:35:39.569042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Joining Tokens into Sentences","metadata":{}},{"cell_type":"code","source":"train['text_cleaned'] = train['text_lemmatized'].str.join(' ')\nval['text_cleaned'] = val['text_lemmatized'].str.join(' ')\n\ntrain.head() # Final form of the dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:37:13.218602Z","iopub.execute_input":"2023-06-05T14:37:13.219087Z","iopub.status.idle":"2023-06-05T14:37:13.269163Z","shell.execute_reply.started":"2023-06-05T14:37:13.219056Z","shell.execute_reply":"2023-06-05T14:37:13.267424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WordCloud = WordCloud(max_words=100,\n                      random_state=30,\n                      collocations=True).generate(str((train['text_cleaned'])))\n\nplt.figure(figsize=(15, 8))\nplt.imshow(WordCloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:37:41.931292Z","iopub.execute_input":"2023-06-05T14:37:41.931716Z","iopub.status.idle":"2023-06-05T14:37:42.453292Z","shell.execute_reply.started":"2023-06-05T14:37:41.931687Z","shell.execute_reply":"2023-06-05T14:37:42.451742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing with Tensorflow","metadata":{}},{"cell_type":"code","source":"num_words = 10000\ntokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(train['text_cleaned'])\n\nword_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:38:30.703056Z","iopub.execute_input":"2023-06-05T14:38:30.703573Z","iopub.status.idle":"2023-06-05T14:38:31.055084Z","shell.execute_reply.started":"2023-06-05T14:38:30.703476Z","shell.execute_reply":"2023-06-05T14:38:31.053679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tokenized_train = tokenizer.texts_to_sequences(train['text_cleaned'])\nTokenized_val = tokenizer.texts_to_sequences(val['text_cleaned'])","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:38:49.077287Z","iopub.execute_input":"2023-06-05T14:38:49.077741Z","iopub.status.idle":"2023-06-05T14:38:49.393457Z","shell.execute_reply.started":"2023-06-05T14:38:49.077708Z","shell.execute_reply":"2023-06-05T14:38:49.392605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Padding","metadata":{}},{"cell_type":"code","source":"maxlen = 40\nPadded_train = pad_sequences(Tokenized_train, maxlen=maxlen, padding='pre')\nPadded_val = pad_sequences(Tokenized_val, maxlen=maxlen, padding='pre')\n\nprint('Non-padded Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][0]]))\nprint('Padded Version: ', Padded_train[0])\nprint('--'*50)\nprint('Non-padded Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][10]]))\nprint('Padded Version: ', Padded_train[10])","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:39:26.179717Z","iopub.execute_input":"2023-06-05T14:39:26.180288Z","iopub.status.idle":"2023-06-05T14:39:26.281663Z","shell.execute_reply.started":"2023-06-05T14:39:26.180240Z","shell.execute_reply":"2023-06-05T14:39:26.280756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(num_words, 16, input_length=maxlen))\nmodel.add(GlobalAvgPool1D())\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, activation='relu'))\nmodel.add(Dropout(0.3))\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.3))\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:40:14.747683Z","iopub.execute_input":"2023-06-05T14:40:14.748175Z","iopub.status.idle":"2023-06-05T14:40:15.069673Z","shell.execute_reply.started":"2023-06-05T14:40:14.748142Z","shell.execute_reply":"2023-06-05T14:40:15.068223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 20\nhist = model.fit(Padded_train, train['label'], epochs=epochs,\n                 validation_data=(Padded_val, val['label']), \n                 )","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:41:51.536981Z","iopub.execute_input":"2023-06-05T14:41:51.537425Z","iopub.status.idle":"2023-06-05T14:42:30.440843Z","shell.execute_reply.started":"2023-06-05T14:41:51.537393Z","shell.execute_reply":"2023-06-05T14:42:30.439480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validation Loss Graphs","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5, 4))\nplt.plot(hist.history['loss'], label='Train Loss')\nplt.plot(hist.history['val_loss'], label='Validation Loss')\nplt.title('Train and Validation Loss Graphs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:43:40.937229Z","iopub.execute_input":"2023-06-05T14:43:40.938045Z","iopub.status.idle":"2023-06-05T14:43:41.377075Z","shell.execute_reply.started":"2023-06-05T14:43:40.937857Z","shell.execute_reply":"2023-06-05T14:43:41.375833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Test Data","metadata":{}},{"cell_type":"code","source":"test['text_tokenized'] = test['text'].apply(tokenization)\ntest['text_stop'] = test['text_tokenized'].apply(stopwords_remove)\ntest['text_lemmatized'] = test['text_stop'].apply(lemmatization)\ntest['text_cleaned'] = test['text_lemmatized'].str.join(' ')\n\nTokenized_test = tokenizer.texts_to_sequences(test['text_cleaned'])\nPadded_test = pad_sequences(Tokenized_test, maxlen=maxlen, padding='pre')\n\ntest_evaluate = model.evaluate(Padded_test, test['label'])","metadata":{"execution":{"iopub.status.busy":"2023-06-05T14:44:47.737511Z","iopub.execute_input":"2023-06-05T14:44:47.738238Z","iopub.status.idle":"2023-06-05T14:44:48.745720Z","shell.execute_reply.started":"2023-06-05T14:44:47.738190Z","shell.execute_reply":"2023-06-05T14:44:48.744275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Predictions in the Test Data","metadata":{}},{"cell_type":"code","source":"labels_dict = {0:'sadness', 1:'joy', 2:'love', 3:'anger', 4:'fear', 5:'surprise'}\ntest['description'] = test['label'].map(labels_dict )","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:06:31.374952Z","iopub.execute_input":"2023-06-05T15:06:31.375492Z","iopub.status.idle":"2023-06-05T15:06:31.383584Z","shell.execute_reply.started":"2023-06-05T15:06:31.375456Z","shell.execute_reply":"2023-06-05T15:06:31.382565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:06:33.401284Z","iopub.execute_input":"2023-06-05T15:06:33.402509Z","iopub.status.idle":"2023-06-05T15:06:33.425221Z","shell.execute_reply.started":"2023-06-05T15:06:33.402469Z","shell.execute_reply":"2023-06-05T15:06:33.423882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(text_input):\n    text_input = str(text_input)\n    text_input = tokenization(text_input)\n    text_input = stopwords_remove(text_input)\n    text_input = lemmatization(text_input)\n    text_input = ' '.join(text_input)\n    text_input = tokenizer.texts_to_sequences([text_input])\n    text_input = pad_sequences(text_input, maxlen=maxlen, padding='pre')\n    text_input = np.argmax(model.predict(text_input))\n    \n    if text_input == 0:\n        print('Predicted Emotion: Sadness')\n    elif text_input == 1:\n        print('Predicted Emotion: Joy')\n    elif text_input == 2:\n        print('Predicted Emotion: Love')\n    elif text_input == 3:\n        print('Predicted Emotion: Anger')\n    elif text_input == 4:\n        print('Predicted Emotion: Fear')\n    else:\n        print('Predicted Emotion: Surprise')\n    return text_input\n\nimport random\n# Randomly chosen Test Dataset data points\ni = random.randint(0, len(test) - 1)\n\nprint('Test Text:', test['text'][i])\nprint(' ')\nprint('Actual Emotion:', test['description'][i])\nmake_predictions(test['text'][i])\nprint('-'*50)\nprint('Test Text:', test['text'][i+1])\nprint(' ')\nprint('Actual Emotion:', test['description'][i+1])\nmake_predictions(test['text'][i+1])","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:07:20.776398Z","iopub.execute_input":"2023-06-05T15:07:20.776885Z","iopub.status.idle":"2023-06-05T15:07:21.056450Z","shell.execute_reply.started":"2023-06-05T15:07:20.776855Z","shell.execute_reply":"2023-06-05T15:07:21.055221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some Fun","metadata":{}},{"cell_type":"code","source":"make_predictions(\"Grandpa was very proud of me when I got a promotion at work. He took me out to dinner to celebrate.\")","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:12:57.600280Z","iopub.execute_input":"2023-06-05T15:12:57.600807Z","iopub.status.idle":"2023-06-05T15:12:57.681254Z","shell.execute_reply.started":"2023-06-05T15:12:57.600775Z","shell.execute_reply":"2023-06-05T15:12:57.680338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(\"Sometimes the people who appear to be the most confident are actually afraid of their own shadows.\")","metadata":{"execution":{"iopub.status.busy":"2023-06-05T15:14:10.075045Z","iopub.execute_input":"2023-06-05T15:14:10.075466Z","iopub.status.idle":"2023-06-05T15:14:10.157899Z","shell.execute_reply.started":"2023-06-05T15:14:10.075438Z","shell.execute_reply":"2023-06-05T15:14:10.156813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}